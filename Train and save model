import pandas as pd
import joblib
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import randint

# Load the dataset
df = pd.read_csv('C:/Users/DELL-7420/OneDrive/Documents/Tensile.csv')

# Preprocessing
# Encode categorical features
label_encoder = LabelEncoder()
df['Material'] = label_encoder.fit_transform(df['Material'])
df['Infill Pattern'] = label_encoder.fit_transform(df['Infill Pattern'])
df['Raster Orientation'] = label_encoder.fit_transform(df['Raster Orientation'])

# Feature scaling
scaler = StandardScaler()
df['Infill Density'] = scaler.fit_transform(df[['Infill Density']])

# Split the data into features and target
X = df[['Material', 'Infill Density', 'Infill Pattern', 'Raster Orientation']]
y = df[['Tensile Strength', 'Hardness', 'Compressive Strength']]

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Set up the RandomForestRegressor model
rf = RandomForestRegressor(random_state=42)

# Define the hyperparameter grid for RandomizedSearchCV
param_dist = {
    'n_estimators': randint(100, 1000),  # Number of trees in the forest
    'max_depth': randint(5, 30),         # Maximum depth of the tree
    'min_samples_split': randint(2, 10), # Minimum samples to split a node
    'min_samples_leaf': randint(1, 10),  # Minimum samples per leaf node
    'max_features': ['auto', 'sqrt'],    # Number of features to consider at each split
}

# Use RandomizedSearchCV for optimization
random_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_dist,
    n_iter=100,  # Number of random combinations to try
    cv=5,        # 5-fold cross-validation
    scoring='r2',  # Optimize for R² score
    verbose=2,
    random_state=42,
    n_jobs=-1  # Use all processors
)

# Fit RandomizedSearchCV to the data
random_search.fit(X_train, y_train)

# Get the best model
best_rf = random_search.best_estimator_

# Make predictions using the best model
y_pred = best_rf.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Best Parameters: {random_search.best_params_}")
print(f"Mean Squared Error: {mse}")
print(f"R² Score: {r2}")

# Save the best model
joblib.dump(best_rf, 'optimized_xgboost_model.pkl')
print("Optimized XGBoost model saved!")
